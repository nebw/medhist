{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import numba\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for testing\n",
    "def generate_synthetic_test_dataset(\n",
    "        num_individuals = 10000,\n",
    "        num_records = 1000,\n",
    "        num_endpoints = 100,\n",
    "        mean_observation_time = 10\n",
    "):\n",
    "    # simulate records with varying frequencies\n",
    "    record_frequencies = np.random.rand(num_records)\n",
    "    records = [np.random.randn(num_individuals, 1) < freq for freq in record_frequencies]\n",
    "    records = np.concatenate(records, axis=1)\n",
    "\n",
    "    observation_times = np.random.randn(num_individuals) + mean_observation_time\n",
    "    observation_times = np.clip(observation_times, 1, None)\n",
    "\n",
    "    # simulate events with varying frequencies\n",
    "    # exponential distribution lambdas with mean event times in [0, 200] years \n",
    "    lambdas_inv = np.random.rand(1, num_endpoints)\n",
    "    # make lambdas depend on records (so that the model can learn something)\n",
    "    weights = np.random.randn(num_records, num_endpoints)\n",
    "    lambdas_pers_inv = np.clip(lambdas_inv * 200 + (records @ weights), 0, None)\n",
    "    lambdas = 1 / (lambdas_pers_inv + 0.001)\n",
    "    # sample random event times for each person and endpoint\n",
    "    label_times = np.random.exponential(scale=1 / lambdas, size=(num_individuals, num_endpoints))\n",
    "    # censoring\n",
    "    label_events = label_times < observation_times[:, None]\n",
    "\n",
    "    censorings = observation_times\n",
    "    # assume no prior events for synthetic data\n",
    "    exclusions = np.zeros((num_individuals, num_endpoints), dtype=bool)\n",
    "\n",
    "    endpoint_names = [f'endpoint_{i}' for i in range(num_endpoints)]\n",
    "\n",
    "    records = scipy.sparse.csr_matrix(records)\n",
    "    label_times = scipy.sparse.csr_matrix(label_times)\n",
    "    label_events = scipy.sparse.csr_matrix(label_events)\n",
    "    exclusions = scipy.sparse.csr_matrix(exclusions)\n",
    "\n",
    "    return records, label_events, label_times, censorings, exclusions, endpoint_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, label_events, label_times, censorings, exclusions, endpoint_names = generate_synthetic_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1000), (10000, 100), (10000, 100), (10000,), (10000, 100))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.shape, label_events.shape, label_times.shape, censorings.shape, exclusions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoxPHLoss(torch.nn.Module):\n",
    "    def forward(self, logh, durations, events, eps=1e-7):\n",
    "        batch_size, endpoints = durations.shape\n",
    "        order = durations.sort(descending=True, dim=0)[1]\n",
    "        offset = torch.tensor(\n",
    "            [order.shape[0] * i for i in range(order.shape[1])], device=order.device\n",
    "        )\n",
    "        order = (order + offset[None, :]).flatten()\n",
    "        f_reorder = lambda arr: rearrange(\n",
    "            rearrange(arr, \"b e -> (e b)\", b=batch_size)[order], \"(b e) -> b e\", b=batch_size\n",
    "        )\n",
    "        logh = f_reorder(logh)\n",
    "        events = f_reorder(events)\n",
    "        gamma = logh.max(0)[0]\n",
    "        log_cumsum_h = logh.sub(gamma).exp().cumsum(0).add(eps).log().add(gamma)\n",
    "        s_sum = events.sum(0)\n",
    "        s_sum[s_sum == 0] = 1\n",
    "        return -logh.sub(log_cumsum_h).mul(events).sum(0).div(s_sum)\n",
    "\n",
    "\n",
    "@numba.njit(parallel=False, nogil=True)\n",
    "def cindex(events, event_times, predictions):\n",
    "    idxs = np.argsort(event_times)\n",
    "\n",
    "    events = events[idxs]\n",
    "    event_times = event_times[idxs]\n",
    "    predictions = predictions[idxs]\n",
    "\n",
    "    n_concordant = 0\n",
    "    n_comparable = 0\n",
    "\n",
    "    for i in numba.prange(len(events)):\n",
    "        for j in range(i + 1, len(events)):\n",
    "            if events[i] and events[j]:\n",
    "                n_comparable += 1\n",
    "                n_concordant += (event_times[i] > event_times[j]) == (\n",
    "                    predictions[i] > predictions[j]\n",
    "                )\n",
    "            elif events[i]:\n",
    "                n_comparable += 1\n",
    "                n_concordant += predictions[i] < predictions[j]\n",
    "\n",
    "    if n_comparable > 0:\n",
    "        return n_concordant / n_comparable\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading medical records data.\n",
    "\n",
    "    Args:\n",
    "        records (scipy.sparse.csr_matrix): Sparse matrix of medical records.\n",
    "        exclusions (scipy.sparse.csr_matrix): Sparse matrix of exclusions.\n",
    "        labels_events (scipy.sparse.csr_matrix): Sparse matrix of event labels.\n",
    "        labels_times (scipy.sparse.csr_matrix): Sparse matrix of time labels.\n",
    "        censorings (Optional[np.array], optional): Array of censoring times. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        records: scipy.sparse.csr_matrix,\n",
    "        exclusions: scipy.sparse.csr_matrix,\n",
    "        labels_events: scipy.sparse.csr_matrix,\n",
    "        labels_times: scipy.sparse.csr_matrix,\n",
    "        censorings: Optional[np.array] = None,\n",
    "    ):\n",
    "        self.records = records\n",
    "        self.exclusions = exclusions\n",
    "        self.labels_events = labels_events\n",
    "        self.labels_times = labels_times\n",
    "        self.censorings = censorings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.records.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        records = torch.Tensor(self.records[idx].todense()).squeeze().bool().float()\n",
    "        exclusions = torch.Tensor(self.exclusions[idx].todense()).squeeze()\n",
    "        labels_events = torch.Tensor(self.labels_events[idx].todense()).squeeze()\n",
    "        labels_times = torch.Tensor(self.labels_times[idx].todense()).squeeze()\n",
    "\n",
    "        censorings = None\n",
    "        if self.censorings is not None:\n",
    "            if not isinstance(idx, list):\n",
    "                idx = [idx]\n",
    "            censorings = torch.Tensor(self.censorings[idx]).squeeze()\n",
    "\n",
    "        return dict(\n",
    "            records=records,\n",
    "            labels_events=labels_events,\n",
    "            labels_times=labels_times,\n",
    "            censorings=censorings,\n",
    "            exclusions=exclusions,\n",
    "        )\n",
    "\n",
    "\n",
    "class RecordsDataModule(LightningDataModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning DataModule for loading medical records data.\n",
    "\n",
    "    Args:\n",
    "        records (scipy.sparse.csr_matrix): Sparse matrix of medical records.\n",
    "        exclusions (scipy.sparse.csr_matrix): Sparse matrix of exclusions.\n",
    "        labels_events (scipy.sparse.csr_matrix): Sparse matrix of event labels.\n",
    "        labels_times (scipy.sparse.csr_matrix): Sparse matrix of time labels.\n",
    "        censorings (Optional[np.array], optional): Array of censoring times. Defaults to None.\n",
    "        indices (Tuple[np.array, np.array, np.array]): Tuple of train, validation, and test indices.\n",
    "        batch_size (int): Batch size for data loading.\n",
    "\n",
    "    Attributes:\n",
    "        data_train (RecordsDataset): Training dataset.\n",
    "        data_val (RecordsDataset): Validation dataset.\n",
    "        data_test (RecordsDataset): Test dataset.\n",
    "\n",
    "    Methods:\n",
    "        setup(stage: str): Splits the data into train, validation, and test sets.\n",
    "        train_dataloader(): Returns a DataLoader for the training set.\n",
    "        val_dataloader(): Returns a DataLoader for the validation set.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, records, label_events, label_times, exclusions, censorings, indices, batch_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.records = records\n",
    "        self.label_events = label_events\n",
    "        self.label_times = label_times\n",
    "        self.exclusions = exclusions\n",
    "        self.censorings = censorings\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        train_idxs, val_idxs, test_idxs = self.indices\n",
    "\n",
    "        self.data_train = RecordsDataset(\n",
    "            self.records[train_idxs],\n",
    "            self.exclusions[train_idxs],\n",
    "            self.label_events[train_idxs],\n",
    "            self.label_times[train_idxs],\n",
    "            self.censorings[train_idxs],\n",
    "        )\n",
    "\n",
    "        self.data_val = RecordsDataset(\n",
    "            self.records[val_idxs],\n",
    "            self.exclusions[val_idxs],\n",
    "            self.label_events[val_idxs],\n",
    "            self.label_times[val_idxs],\n",
    "            self.censorings[val_idxs],\n",
    "        )\n",
    "\n",
    "        self.data_test = RecordsDataset(\n",
    "            self.records[test_idxs],\n",
    "            self.exclusions[test_idxs],\n",
    "            self.label_events[test_idxs],\n",
    "            self.label_times[test_idxs],\n",
    "            self.censorings[test_idxs],\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data_train, batch_size=self.batch_size, drop_last=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.data_val, batch_size=self.batch_size, drop_last=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.data_test, batch_size=self.batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalHistoryModule(LightningModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning module for training a medical history model using Cox proportional hazards loss.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The medical history model to train.\n",
    "        endpoint_names (list): A list of endpoint names for the model.\n",
    "        exclusions_on_metrics (bool, optional): Whether to exclude samples with exclusions when computing metrics. Defaults to True.\n",
    "        lr (float, optional): The learning rate for the optimizer. Defaults to 0.005.\n",
    "        weight_decay (float, optional): The weight decay for the optimizer. Defaults to 0.01.\n",
    "        n_chunks (int, optional): The number of chunks to split the data into when computing the loss. Defaults to 1.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        endpoint_names: list,\n",
    "        exclusions_on_metrics: bool = True,\n",
    "        lr: float = 0.005,\n",
    "        weight_decay: float = 0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = CoxPHLoss()\n",
    "\n",
    "        self.model = model\n",
    "        self.endpoint_names = endpoint_names\n",
    "        self.num_endpoints = len(endpoint_names)\n",
    "        self.exclusions_on_metrics = exclusions_on_metrics\n",
    "\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.executor = ThreadPoolExecutor(max_workers=multiprocessing.cpu_count())\n",
    "        self.max_mean_cindex = 0\n",
    "\n",
    "        self.valid_data = [([], [], []) for _ in range(self.num_endpoints)]\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "\n",
    "    def get_loss(self, batch, predictions):\n",
    "        ehr_events = batch[\"labels_events\"].bool()\n",
    "        ehr_times = batch[\"labels_times\"]\n",
    "        ehr_censorings = batch[\"censorings\"][:, None].repeat(1, self.num_endpoints)\n",
    "        # set event time to censoring time for non-events\n",
    "        ehr_times[~ehr_events] = ehr_censorings[~ehr_events]\n",
    "\n",
    "        losses = self.loss(predictions, ehr_times.squeeze(), ehr_events.squeeze())\n",
    "        loss = torch.mean(losses)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def shared_step(self, batch, batch_idx):\n",
    "        ehr_records = batch[\"records\"]\n",
    "        ehr_records = ehr_records.bool().float()\n",
    "\n",
    "        latents, predictions = self.model(ehr_records)\n",
    "\n",
    "        return latents, predictions, self.get_loss(batch, predictions)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        latents, predictions, loss = self.shared_step(batch, batch_idx)\n",
    "        self.log(\"train/loss\", loss.item(), batch_size=len(predictions))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        latents, predictions, loss = self.shared_step(batch, batch_idx)\n",
    "\n",
    "        self.log(\"valid/loss\", loss.item(), batch_size=len(predictions))\n",
    "\n",
    "        events = batch[\"labels_events\"].detach().cpu()\n",
    "        times = batch[\"labels_times\"].detach().cpu()\n",
    "        exclusions = batch[\"exclusions\"].detach().cpu()\n",
    "\n",
    "        for endpoint_idx in range(self.num_endpoints):\n",
    "            if self.exclusions_on_metrics:\n",
    "                mask = exclusions[:, endpoint_idx] == 0\n",
    "\n",
    "                predictions_ = predictions[mask, endpoint_idx]\n",
    "                events_ = events[mask, endpoint_idx]\n",
    "                times_ = times[mask, endpoint_idx]\n",
    "            else:\n",
    "                predictions_ = predictions[:, endpoint_idx]\n",
    "                events_ = events[:, endpoint_idx]\n",
    "                times_ = times[:, endpoint_idx]\n",
    "\n",
    "            self.valid_data[endpoint_idx][0].append(predictions_.detach().cpu().float().numpy())\n",
    "            self.valid_data[endpoint_idx][1].append(events_.numpy())\n",
    "            self.valid_data[endpoint_idx][2].append(times_.numpy())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        def compute(valid_data):\n",
    "            preds = np.concatenate(valid_data[0]).squeeze()\n",
    "            events = np.concatenate(valid_data[1]).astype(bool)\n",
    "            times = np.concatenate(valid_data[2]).squeeze()\n",
    "\n",
    "            return cindex(events, times, 1 - preds)\n",
    "\n",
    "        cindices = list(self.executor.map(compute, self.valid_data))\n",
    "        for endpoint_idx in range(self.num_endpoints):\n",
    "            cidx = cindices[endpoint_idx]\n",
    "            endpoint_name = (\n",
    "                self.endpoint_names[endpoint_idx] if self.endpoint_names else endpoint_idx\n",
    "            )\n",
    "            self.log(f\"valid/cindex_{endpoint_name}\", cidx)\n",
    "\n",
    "        self.valid_data = [([], [], []) for _ in range(self.num_endpoints)]\n",
    "\n",
    "        self.log(f\"valid/mean_cindex\", np.nanmean(cindices))\n",
    "\n",
    "        self.max_mean_cindex = max(np.nanmean(cindices), self.max_mean_cindex)\n",
    "        self.log(f\"valid/mean_cindex_max\", self.max_mean_cindex)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (torch.nn.LayerNorm, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)):\n",
    "            torch.nn.init.constant_(m.weight, 1)\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (torch.nn.Conv2d, torch.nn.Conv3d)):\n",
    "            torch.nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        elif isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalHistoryModel(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hidden, dropout_input, dropout_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(dropout_input),\n",
    "            torch.nn.Linear(num_inputs, num_hidden),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LayerNorm(num_hidden),\n",
    "\n",
    "            torch.nn.Dropout(dropout_hidden),\n",
    "            torch.nn.Linear(num_hidden, num_hidden),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LayerNorm(num_hidden),\n",
    "\n",
    "            torch.nn.Dropout(dropout_hidden),\n",
    "        )\n",
    "\n",
    "        # Define the output layer\n",
    "        self.head = torch.nn.Linear(num_hidden, num_outputs, bias=False)\n",
    "\n",
    "    def forward(self, records):\n",
    "        # Compute the latent representation of the input records\n",
    "        latents = self.model(records)\n",
    "\n",
    "        # Compute the model predictions\n",
    "        predictions = self.head(latents)\n",
    "\n",
    "        return latents, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size for training\n",
    "batch_size = 2048\n",
    "\n",
    "# Get the number of inputs and outputs for the model\n",
    "num_inputs = records.shape[1]\n",
    "num_outputs = len(endpoint_names)\n",
    "\n",
    "# Set the number of hidden units and dropout rates for the model\n",
    "num_hidden = 4096\n",
    "dropout_input = 0.18\n",
    "dropout_hidden = 0.85\n",
    "\n",
    "# Initialize a MedicalHistoryModel object with the specified settings\n",
    "model = MedicalHistoryModel(num_inputs, num_outputs, num_hidden, dropout_input, dropout_hidden)\n",
    "\n",
    "# Initialize a MedicalHistoryModule object with the specified settings\n",
    "module = MedicalHistoryModule(model, endpoint_names, lr=0.000628, weight_decay=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random train, valid, test split of data\n",
    "num_individuals = records.shape[0]\n",
    "\n",
    "idxs = np.arange(num_individuals)\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "train_idxs = idxs[: int(0.8 * num_individuals)]\n",
    "val_idxs = idxs[int(0.8 * num_individuals) : int(0.9 * num_individuals)]\n",
    "test_idxs = idxs[int(0.9 * num_individuals) :]\n",
    "\n",
    "indices = (train_idxs, val_idxs, test_idxs)\n",
    "\n",
    "data = RecordsDataModule(\n",
    "    records,\n",
    "    label_events,\n",
    "    label_times,\n",
    "    exclusions,\n",
    "    censorings,\n",
    "    indices,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnebw\u001b[0m (\u001b[33mcardiors\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240105_132133-1hkxnnrr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cardiors/medhist_simulated/runs/1hkxnnrr' target=\"_blank\">240105_simulated_test</a></strong> to <a href='https://wandb.ai/cardiors/medhist_simulated' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cardiors/medhist_simulated' target=\"_blank\">https://wandb.ai/cardiors/medhist_simulated</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cardiors/medhist_simulated/runs/1hkxnnrr' target=\"_blank\">https://wandb.ai/cardiors/medhist_simulated/runs/1hkxnnrr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "/sc-projects/sc-proj-ukb-cvd/environments/phenomenal/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /sc-projects/sc-proj-ukb-cvd/environments/phenomenal ...\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/sc-projects/sc-proj-ukb-cvd/environments/phenomenal/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /sc-projects/sc-proj-ukb-cvd/environments/phenomenal ...\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | loss  | CoxPHLoss           | 0     \n",
      "1 | model | MedicalHistoryModel | 21.3 M\n",
      "----------------------------------------------\n",
      "21.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.3 M    Total params\n",
      "85.230    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65503b81f4e4fecb632fec0ced1ee53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sc-projects/sc-proj-ukb-cvd/environments/phenomenal/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:109: UserWarning: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "  rank_zero_warn(\n",
      "/sc-projects/sc-proj-ukb-cvd/environments/phenomenal/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/sc-projects/sc-proj-ukb-cvd/environments/phenomenal/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f97e5b85c064c62bbd2b03b1f408368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sc-projects/sc-proj-ukb-cvd/environments/phenomenal/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:360: UserWarning: `ModelCheckpoint(monitor='valid/mean_cindex')` could not find the monitored key in the returned metrics: ['train/loss', 'epoch', 'step']. HINT: Did you call `log('valid/mean_cindex', value)` in the `LightningModule`?\n",
      "  warning_cache.warn(m)\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error uploading \"/home/wildb/.local/share/wandb/artifacts/staging/tmpm4yrgw63\": OSError, [Errno 28] No space left on device: '/sc-scratch/sc-scratch-ukb-cvd/.cache/wandb/artifacts/obj/md5/99/tmp_AFE6C0FF'\n",
      "wandb: ERROR Uploading artifact file failed. Artifact won't be committed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>lr-AdamW</td><td>▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▅▅██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>lr-AdamW</td><td>0.00063</td></tr><tr><td>train/loss</td><td>73.86647</td></tr><tr><td>trainer/global_step</td><td>29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">240105_simulated_test</strong> at: <a href='https://wandb.ai/cardiors/medhist_simulated/runs/1hkxnnrr' target=\"_blank\">https://wandb.ai/cardiors/medhist_simulated/runs/1hkxnnrr</a><br/>Synced 7 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240105_132133-1hkxnnrr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get today's date in YYMMDD format\n",
    "date = datetime.date.today().strftime(\"%y%m%d\")\n",
    "\n",
    "# Set the name of the run\n",
    "run_name = \"simulated_test\"\n",
    "\n",
    "# Initialize a WandbLogger object with the specified settings\n",
    "wandb_logger = WandbLogger(\n",
    "    name=f\"{date}_{run_name}\",\n",
    "    project=\"medhist_simulated\",\n",
    "    log_model=True,\n",
    "    settings=wandb.Settings(start_method=\"thread\"),\n",
    "    notes=repr(model),\n",
    ")\n",
    "\n",
    "# Watch the model for logging purposes\n",
    "wandb_logger.watch(model, log=\"all\")\n",
    "\n",
    "# Initialize a Trainer object with the specified settings\n",
    "trainer = Trainer(\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"valid/mean_cindex\", mode=\"max\", min_delta=1e-8, patience=20),\n",
    "        ModelCheckpoint(mode=\"max\", monitor=\"valid/mean_cindex\", save_top_k=1, save_last=True),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ],\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=0.25,\n",
    "    accelerator=\"auto\", \n",
    "    devices=\"auto\", \n",
    "    strategy=\"auto\",\n",
    "    max_epochs=10,\n",
    ")\n",
    "\n",
    "# Train the model using the Trainer object and the LightningModule and LightningDataModule objects\n",
    "trainer.fit(module, datamodule=data)\n",
    "\n",
    "# Finish the logging process\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
